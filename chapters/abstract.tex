% !TeX root = ../main.tex

\ustcsetup{
  keywords = {
    Poolingformer,Transformers ,注意力,长文本,摘要,相对位置编码
  },
  keywords* = {
    Poolingformer,Transformers ,Attention,Long Document,Summarization,Relative Position Embedding
  },
}

\begin{abstract}

基于Transformers的模型在面对长文本任务时往往会因为其序列长度的平方复杂度而产生庞大的计算开销，为了解决这个问题，本论文提出了一种具有双层稀疏注意力机制的改进模型Poolingformer。模型的第一层注意力采用了滑动窗口注意力机制，每个token只和其临近的接收窗口内的token计算注意力；而在额外的第二层注意力中，在接收窗口比第一层更大的同时，模型还通过池化操作压缩了序列长度，所以模型在获得更多信息的同时减少了计算量。除此之外，本文还尝试将Deberta模型的相对位置编码机制应用到Poolingformer里。本文在长文本摘要任务的arXiv与PubMed数据集上测试了Poolingformer，并取得了同期最佳的结果。这充分说明了Poolingformer能够高效率地利用、发掘文本中的信息。

\end{abstract}

\begin{abstract*}

For models based on Transformers, their computational and memory complexity tend to grow quadratically with respect to sequence length. To solve this problem, we introduce Poolingformer which revises full attention schema to a two-level sparse attention schema. Its first layer adopts a sliding window attention in which every token attend to their neighbors in the receptive window. In the additional second layer of attention computing, model applies pooling to compress sequence length as long as the larger window size. By doing so, model can achieve more information gain at a relatively low computation cost. Beside of above, we tried to apply Deberta's relative attention to Poolingformer to create a combined model.For experiments, 
we evaluate Poolingformer on long sequence summarization task, and achieved state-of-the-art performance on both arXiv and PubMed dataset. This fully demonstrates Poolingformer's outtanding ability to utilize the information.

\end{abstract*}