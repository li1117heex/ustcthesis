\chapter{结论}

本论文提出了Poolingformer，一种基于创新的双层自注意力机制的针对长文本的模型，且实现了线性复杂度。其第一层注意力采用了滑动窗口注意力机制，每个token只和全局token与其临近的接收窗口内的token计算注意力。而在额外的第二层注意力中，在接收窗口比第一层更大的同时，模型还通过池化操作压缩了序列长度，所以模型在获得更多信息的同时减少了计算量。在长文本摘要任务上，Poolingformer在arXiv与PubMed数据集上都取得了同期最佳的结果。此外，本文还在Poolingformer上融合了Deberta的相对位置编码机制，并通过预训练与问答任务说明了该融合模型的潜力。Poolingformer的模型与结构值得未来的工作进行更多的研究探索，包括其在其他长文本任务上的表现，又或者再加入更多层注意力的模型表现如何等等。